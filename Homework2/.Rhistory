m1a$null.deviance - m1a$dev
Anova(m1)
Anova(m1a)
Anova(m1a, test="Chisq")
Anova(m1a, test="LR")
Anova(m1, test="LR")
View(dfage)
View(elecc92)
View(dfage)
View(dfage)
View(elecc92)
llwork<- sample(1:nrow(elecc92), round(0.8*nrow(elecc92), dig=0))
df <- elecc92[llwork,]
dftest <- elecc92[-llwork,]
m2<- glm(pres~age+edduc, family=binomial, data=elecc92)
summary(m2)
m2<- glm(pres~age+edduc, family=binomial, data=elecc92)
m2<- glm(pres~age+educ, family=binomial, data=elecc92)
summary(m2)
Anova(m2, test="LR")
residualPlots(m2)
marginalModelPlots(m2)
m3<- glm(pres~poly(age,2) + poly(educ,3), family=binomial, data=dfwork)
m2<- glm(pres~age+educ, family=binomial, data=dfwork)
summary(m2)
m2<- glm(pres~age+educ, family=binomial, data=dfwork)
set.seed(1234)
llwork<- sample(1:nrow(elecc92), round(0.8*nrow(elecc92), dig=0))
dfwork <- elecc92[llwork,]
dftest <- elecc92[-llwork,]
## Modeling with numeric variables with dfwork
```{r}
m2<- glm(pres~age+educ, family=binomial, data=dfwork)
summary(m2)
Anova(m2, test="LR") #the net effect of age/education is significant once education/age is already in the model, so both are useful.
residualPlots(m2)
marginalModelPlots(m2)
m3<- glm(pres~poly(age,2) + poly(educ,3), family=binomial, data=dfwork)
m3<- glm(pres~poly(age,2) + poly(educ,3), family=binomial, data=dfwork)
summary(m3)
residualPlots(m3)
marginalModelPlots(m3)
residualPlots(m3)
marginalModelPlots(m3)
m4<- glm(pres~poly(age,2)*c.educ, family=binomial, data=dfwork)
m4<- glm(pres~poly(age,2)*educ, family=binomial, data=dfwork)
summary(m4)
View(dfwork)
m5<- glm(pres~c.age*educ, family=binomial, data=dfwork) #when we have a really "non-linear" thing, it might b useful to use the factor.
summary(m5)
AIC(m3,m4,m5)
m4<- glm(pres~poly(age,2)*c.edu, family=binomial, data=dfwork)
summary(m4)
m5<- glm(pres~c.age*c.edu, family=binomial, data=dfwork) #when we have a really "non-linear" thing, it might b useful to use the factor.
summary(m5)
AIC(m3,m4,m5)
# m3: 2 numeric (age and education)
# m4: contains numeric age^2, interaction with educ -> two parabolas depending on the level of educ
# m5: two factors (age and educ interaction)
# according to this criteria, m4 is the one which works better. This is bc we have seen very non linear behaviour in education, therefore it is good to use the factor.
marginalModelPlots(m4)
residualPlots(m4)
plot(allEffects(m4))
summary(dfwork)
m5<- glm(pres~poly(age,2)*c.edu + inter + close + sat, family=binomial, data=dfwork)
summmary(m5)
m5<- glm(pres~poly(age,2)*c.edu + inter + close + sat, family=binomial, data=dfwork)
summmary(m5)
summary(m5)
anova(m4,m5, test="Chisq")
waldtest(m4,m5,test="Chisq")
# Assess satisfaction of the candidates
coef(m5)
# Assess satisfaction of the candidates
coef(m5)
# assume that we want to test wheter a coefficient(satSat-Yes) is equal to 0.
linearHypothesis(m5, c("satSat-Yes=0"))
linearHypothesis(m5, c("closeClose-Yes=0"))
linearHypothesis(m5, c("satSat-Yes=0", "closeClose-Yes=0"))
# Deviance test
m5aux <- glm(pres~poly(age,2)*c.edu + inter, family=binomial, data=dfwork)
anova(m5aux, m5, test="Chisq")
Anova(m5, test="LR")
summary(df$work)
dfwork$party<- paste(dfwork$p,dfwork$arty )
View(dfwork)
m6 <- glm(pres~poly(age,2)*c.edu + inter+ close+sat+party, family=binomial, data=dfwork)
summarym6
Anova(m6, test="LR")
dfwork$newparty <- dfwork$party
levels(dfwork$party)
dfwork$newparty <- dfwork$party
levels(dfwork$party)
summary(df$work)
dfwork$party<- factor(paste(dfwork$p,dfwork$arty)) # it was previously separated in 2 columns
m6<- glm(pres~poly(age,2)*c.edu + inter+ close+sat+party, family=binomial, data=dfwork)
summarym6
summary(df$work)
dfwork$party<- factor(paste(dfwork$p,dfwork$arty)) # it was previously separated in 2 columns
m6<- glm(pres~poly(age,2)*c.edu + inter+ close+sat+party, family=binomial, data=dfwork)
summary(m6)
Anova(m6, test="LR") #party is worth, so we need to work with it.
# Since there are lots of categories in the party factor, we will group some of them in such a way to reduce some degrees of freedom (this is a trick, don't report it!).
dfwork$newparty <- dfwork$party
levels(dfwork$party)
m7<- glm(pres~poly(age,2)*c.edu + inter+ close+sat+newparty, family=binomial, data=dfwork)
dfwork$newparty <- dfwork$party
levels(dfwork$party)
levels(dfwork$newparty) <- c("weak", "ind", "weak", "strong", "weak", "weak")
m7<- glm(pres~poly(age,2)*c.edu + inter+ close+sat+newparty, family=binomial, data=dfwork)
levels(dfwork$party)
levels(dfwork$newparty) <- c("weak", "ind", "weak", "strong", "strong", "weak", "weak")
dfwork$newparty <- dfwork$party
levels(dfwork$party)
levels(dfwork$newparty) <- c("weak", "ind", "weak", "strong", "strong", "weak", "weak")
m7<- glm(pres~poly(age,2)*c.edu + inter+ close+sat+newparty, family=binomial, data=dfwork)
summary(m7)
anova(m6, m7, test="Chisq")
## DONT DO IT IN THE ASSIGNMENT 2!!
m8<- glm(pres~poly(2, age)*c.edu + poly(age,2)*(inter+close+sat+newparty)^2, family=binomial, data=dfwork)
## DONT DO IT IN THE ASSIGNMENT 2!!
m8<- glm(pres~poly(age, 2)*c.edu + poly(age,2)*(inter+close+sat+newparty)^2, family=binomial, data=dfwork)
m9<-step(m8)
## DONT DO IT IN THE ASSIGNMENT 2!!
m8<- glm(pres~poly(age, 2)*c.edu + poly(age,2)*(inter+close+sat+newparty)^2, family=binomial, data=dfwork)
m9<-step(m8)
m10<-step(m8, k=log(nrow(dfwork)))
summary(m9)
anova(m10,m9, test="Chisq")
# dont use plot(m10)
residualPlots(m10)
#final model has lack of fit, clearly. We have clear outliers in the residuals and also as influent data. The Linear predictor plot is clearly missfitting...
influencePlot(m10)
#final model has lack of fit, clearly. We have clear outliers in the residuals and also as influent data. The Linear predictor plot is clearly missfitting...
influencePlot(m10, id.n = 6)
#final model has lack of fit, clearly. We have clear outliers in the residuals and also as influent data. The Linear predictor plot is clearly missfitting...
influencePlot(m10, id.n = 5)
#951 and 913 are influence regarding cooks distance. Not worried by otherr residuals (relatively close to the -2 to 2)
Boxplot(cooks.distance(m10), id=list(labels=rownames(dfwork)))
# We must remove the ones which are residual outliers+influent!
llrem <- wihch(rownames(dfwork) %in% c("913", "951"))
# We must remove the ones which are residual outliers+influent!
llrem <- which(rownames(dfwork) %in% c("913", "951"))
# We must remove the ones which are residual outliers+influent!
llrem <- which(rownames(dfwork) %in% c("913", "951"));llrem
dfwork <- dfwork[-llrem,]
summary(m10)
m11 <- glm(formula = pres ~ poly(age, 2) + c.edu + inter + newparty +
poly(age, 2):c.edu, family = binomial, data = dfwork)
influencePlot(m11)
Boxplot(cooks.distance(m11), id=list(labels=rownames(dfwork))) # Further suggest the same, 913 and 951 are the mostsevere outliers. This does not mean we need to remove them, maybe they are aligned with the model.
influencePlot(m11)
Boxplot(cooks.distance(m11), id=list(labels=rownames(dfwork)))
# Chunk 1
library(FactoMineR)
library(factoextra)
rm(list=ls())
df <- read.csv("Wholesale customers data.csv", header = TRUE)
?HCPC
setwd("C:/Users/joana/OneDrive/Desktop/Master-Data-Science/1st semester/MVA-Multivariate Analysis/MVA-MultiVariate-Analysis/Homework2")
# Chunk 1
library(FactoMineR)
library(factoextra)
rm(list=ls())
df <- read.csv("Wholesale customers data.csv", header = TRUE)
str(df)
summary(df)
numeric_vars <- c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")
factor_vars <- c("Channel", "Region")
df[numeric_vars] <- lapply(df[numeric_vars], as.numeric)
df[factor_vars] <- lapply(df[factor_vars], as.factor)
# Chunk 2
numerical_to_factor <- function(data, column_name) {
summary_vals <- summary(data[[column_name]])
factor_col <- cut(data[[column_name]],
breaks = c(-Inf, summary_vals[3], Inf),
labels = c("low", "high"))
return(factor_col)
}
for (col in numeric_vars) {
new_col_name <- paste0("f.", tolower(col))
df[[new_col_name]] <- numerical_to_factor(df, col)
}
# Chunk 3
res.mca <- MCA(df[,c(1,2,9:14)], quali.sup = c("Region","Channel"))
summary(res.mca)
plot(res.mca$eig[,2], type = "b", pch = 16,lwd = 2,col = "blue",xlab = "Dimensions",ylab = "Percentage of Inertia", main = "Eigenvalues")
res.mca$var$coord
res.mca$var$contrib
res.mca$var$cos2
res.mca$var$v.test
dimdesc(res.mca)
# Chunk 4
res.hcpc <- HCPC(res.mca, graph=FALSE)
res.hcpc$desc.axes
res.hcpc$call$t$nb.clust
barplot(res.hcpc$call$t$inert.gain[1:10], type = "b")
# Dendrogram and cluster visualisation using fviz
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE)
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2))
pca_result$eig
hcpc_result <- HCPC(pca_result, nb.clust= -1)
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2))
pca_result$eig
hcpc_result <- HCPC(pca_result, nb.clust= 0)
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2))
hcpc_result <- HCPC(pca_result, nb.clust= 1)
hcpc_result$call$t$nb.clust # 3 clusters
?HCPC
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2))
pca_result$eig
hcpc_result <- HCPC(pca_result, nb.clust= -1, min=1)
hcpc_result <- HCPC(pca_result, min=1)
?HCPC
hcpc_result <- HCPC(pca_result, min=2)
hcpc_result <- HCPC(pca_result, min=1)
hcpc_result <- HCPC(pca_result, min=2, nb.clust= -1)
hcpc_result <- HCPC(pca_result, min=2, nb.clust= -1)
hcpc_result <- HCPC(pca_result,graph=FALSE, min=2)
hcpc_result$call$t$nb.clust # 3 clusters
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2))
pca_result$eig
hcpc_result <- HCPC(pca_result,graph=FALSE)
hcpc_result$call$t$nb.clust #3 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
length(which(hcpc_result$data.clust$clust == 3))
hcpc_result <- HCPC(pca_result,graph=FALSE, min=2)
hcpc_result$call$t$nb.clust # 2 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
hcpc_result <- HCPC(pca_result, nb.clust = -1, min=2)
# Set the minimum number of clusters to 2
?HCCCPC
# Set the minimum number of clusters to 2
?HCPC
# Set the minimum number of clusters to 2
hcpc_result <- HCPC(pca_result, min=2)
# Set the minimum number of clusters to 2
hcpc_result <- HCPC(pca_result, min=2, nb.lust = -1)
# Set the minimum number of clusters to 2
hcpc_result <- HCPC(pca_result, min=2, nb.clust = -1)
hcpc_result <- HCPC(pca_result, min=2, nb.clust = -1)
#Limit PCA to the first 2 dimensions
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2), ncp=2)
hcpc_result <- HCPC(pca_result, min=2, nb.clust = -1)
hcpc_result$call$t$nb.clust # 4 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
length(which(hcpc_result$data.clust$clust == 3))
length(which(hcpc_result$data.clust$clust == 4))
View(df)
hcpc_result$desc.var
fviz_cluster(hcpc_result, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
# Set the minimum number of clusters to 2
hcpc_result <- HCPC(pca_result, min=2, nb.clust = -1)
hcpc_result$call$t$nb.clust # 2 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
hcpc_result$desc.var
hcpc_result$call
hcpc_result$$quali.sup
hcpc_result$quali.sup
fviz_cluster(hcpc_result, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
View(df)
res.hcpc$desc.var$category
# Chunk 1
library(FactoMineR)
library(factoextra)
rm(list=ls())
df <- read.csv("Wholesale customers data.csv", header = TRUE)
str(df)
summary(df)
numeric_vars <- c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")
factor_vars <- c("Channel", "Region")
df[numeric_vars] <- lapply(df[numeric_vars], as.numeric)
df[factor_vars] <- lapply(df[factor_vars], as.factor)
# Chunk 2
numerical_to_factor <- function(data, column_name) {
summary_vals <- summary(data[[column_name]])
factor_col <- cut(data[[column_name]],
breaks = c(-Inf, summary_vals[3], Inf),
labels = c("low", "high"))
return(factor_col)
}
for (col in numeric_vars) {
new_col_name <- paste0("f.", tolower(col))
df[[new_col_name]] <- numerical_to_factor(df, col)
}
# Chunk 3
res.mca <- MCA(df[,c(1,2,9:14)], quali.sup = c("Region","Channel"))
summary(res.mca)
plot(res.mca$eig[,2], type = "b", pch = 16,lwd = 2,col = "blue",xlab = "Dimensions",ylab = "Percentage of Inertia", main = "Eigenvalues")
res.mca$var$coord
res.mca$var$contrib
res.mca$var$cos2
res.mca$var$v.test
dimdesc(res.mca)
res.hcpc <- HCPC(res.mca, graph=FALSE)
res.hcpc$desc.axes
res.hcpc$call$t$nb.clust
barplot(res.hcpc$call$t$inert.gain[1:10], type = "b")
# Dendrogram and cluster visualisation using fviz
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE)
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
res.hcpc$desc.var$category
res.hcpc$desc.var
res.mca <- MCA(df[,c(1,2,9:14)], quali.sup = c("Region","Channel"))
res.hcpc <- HCPC(res.mca, graph=FALSE)
res.hcpc <- HCPC(res.mca, graph=TRUE)
res.hcpc <- HCPC(res.mca, nb.clust=-1)
res.hcpc$desc.axes
res.hcpc$call$t$nb.clust
res.mca <- MCA(df[,c(1,2,9:14)], quali.sup = c("Region","Channel"))
summary(res.mca)
plot(res.mca$eig[,2], type = "b", pch = 16,lwd = 2,col = "blue",xlab = "Dimensions",ylab = "Percentage of Inertia", main = "Eigenvalues")
res.mca <- MCA(df[,c(1,2,9:14)], quali.sup = c("Region","Channel"))
res.hcpc <- HCPC(res.mca, nb.clust=-1)
res.hcpc$desc.axes
res.hcpc$call$t$nb.clust
barplot(res.hcpc$call$t$inert.gain[1:10], type = "b")
res.hcpc <- HCPC(res.mca, nb.clust=-1)
res.hcpc$desc.var
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
length(which(res.hcpc$data.clust$clust == 1))
length(which(res.hcpc$data.clust$clust == 2))
length(which(res.hcpc$data.clust$clust == 3))
length(which(res.hcpc$data.clust$clust == 4))
length(which(res.hcpc$data.clust$clust == 3 $$ res.hcpc$data.clust$Channel ==1 ))
length(which(res.hcpc$data.clust$clust == 3 & res.hcpc$data.clust$Channel ==1 ))
res.hcpc$data.clust$Channel
length(which(res.hcpc$data.clust$clust == 3 & res.hcpc$data.clust$Channel == "Channel_1" ))
length(which(res.hcpc$data.clust$clust == 3 & res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$clust == 4 & res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$clust == 4 & res.hcpc$data.clust$Channel == "Channel_1" ))
length(which(res.hcpc$data.clust$clust == 2 & res.hcpc$data.clust$Channel == "Channel_1" ))
length(which(res.hcpc$data.clust$clust == 2 & res.hcpc$data.clust$Channel == "Channel_2" ))
inertia_PCA <- sum(res.hcpc$call$t$inert.gain[1:3]); inertia_PCA
inertia_MCA <- sum(hcpc_result$call$t$inert.gain[1:4]); inertia_MCA
inertia_PCA <- sum(res.hcpc$call$t$inert.gain[1:2]); inertia_PCA
inertia_MCA <- sum(hcpc_result$call$t$inert.gain[1:4]); inertia_MCA
hcpc_result$data.clust
# Chunk 1
library(FactoMineR)
library(factoextra)
rm(list=ls())
df <- read.csv("Wholesale customers data.csv", header = TRUE)
str(df)
summary(df)
numeric_vars <- c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")
factor_vars <- c("Channel", "Region")
df[numeric_vars] <- lapply(df[numeric_vars], as.numeric)
df[factor_vars] <- lapply(df[factor_vars], as.factor)
# Chunk 2
numerical_to_factor <- function(data, column_name) {
summary_vals <- summary(data[[column_name]])
factor_col <- cut(data[[column_name]],
breaks = c(-Inf, summary_vals[3], Inf),
labels = c("low", "high"))
return(factor_col)
}
for (col in numeric_vars) {
new_col_name <- paste0("f.", tolower(col))
df[[new_col_name]] <- numerical_to_factor(df, col)
}
# Chunk 3
res.mca <- MCA(df[,c(1,2,9:14)], quali.sup = c("Region","Channel"))
summary(res.mca)
plot(res.mca$eig[,2], type = "b", pch = 16,lwd = 2,col = "blue",xlab = "Dimensions",ylab = "Percentage of Inertia", main = "Eigenvalues")
res.mca$var$coord
res.mca$var$contrib
res.mca$var$cos2
res.mca$var$v.test
dimdesc(res.mca)
# Chunk 4
res.hcpc <- HCPC(res.mca, nb.clust=-1)
res.hcpc$desc.axes
res.hcpc$call$t$nb.clust
barplot(res.hcpc$call$t$inert.gain[1:10], type = "b")
# Dendrogram and cluster visualisation using fviz
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE)
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
# Chunk 5
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2))
pca_result$eig
hcpc_result <- HCPC(pca_result,graph=FALSE)
hcpc_result$call$t$nb.clust #3 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
length(which(hcpc_result$data.clust$clust == 3))
# Set the minimum number of clusters to 2
hcpc_result <- HCPC(pca_result, min=2, nb.clust = -1)
hcpc_result$call$t$nb.clust # 2 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
fviz_dend(hcpc_result, rect = TRUE, rect_fill = TRUE)
fviz_cluster(hcpc_result, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
#Limit PCA to the first 2 dimensions
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2), ncp=2)
hcpc_result <- HCPC(pca_result, min=2, nb.clust = -1)
hcpc_result$call$t$nb.clust # 4 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
length(which(hcpc_result$data.clust$clust == 3))
length(which(hcpc_result$data.clust$clust == 4))
fviz_dend(hcpc_result, rect = TRUE, rect_fill = TRUE)
# Chunk 6
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
length(which(res.hcpc$data.clust$clust == 2 & res.hcpc$data.clust$Channel == "Channel_1" ))
length(which(res.hcpc$data.clust$clust == 2 & res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$clust == 3 & res.hcpc$data.clust$Channel == "Channel_1" ))
length(which(res.hcpc$data.clust$clust == 3 & res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$clust == 4 & res.hcpc$data.clust$Channel == "Channel_1" ))
res.hcpc$desc.var
res.hcpc$desc.var$category
res.hcpc$data.clust
# Chunk 7
fviz_cluster(hcpc_result, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
hcpc_result$data.clust
hcpc_result$desc.var
hcpc_result$desc.axes
hcpc_result$desc.ind
hcpc_result$call
inertia_PCA <- sum(res.hcpc$call$t$inert.gain[1:2]); inertia_PCA
inertia_MCA <- sum(hcpc_result$call$t$inert.gain[1:4]); inertia_MCA
# Chunk 1
library(FactoMineR)
library(factoextra)
rm(list=ls())
df <- read.csv("Wholesale customers data.csv", header = TRUE)
str(df)
summary(df)
numeric_vars <- c("Fresh", "Milk", "Grocery", "Frozen", "Detergents_Paper", "Delicassen")
factor_vars <- c("Channel", "Region")
df[numeric_vars] <- lapply(df[numeric_vars], as.numeric)
df[factor_vars] <- lapply(df[factor_vars], as.factor)
# Chunk 2
numerical_to_factor <- function(data, column_name) {
summary_vals <- summary(data[[column_name]])
factor_col <- cut(data[[column_name]],
breaks = c(-Inf, summary_vals[3], Inf),
labels = c("low", "high"))
return(factor_col)
}
for (col in numeric_vars) {
new_col_name <- paste0("f.", tolower(col))
df[[new_col_name]] <- numerical_to_factor(df, col)
}
# Chunk 3
res.mca <- MCA(df[,c(1,2,9:14)], quali.sup = c("Region","Channel"))
summary(res.mca)
plot(res.mca$eig[,2], type = "b", pch = 16,lwd = 2,col = "blue",xlab = "Dimensions",ylab = "Percentage of Inertia", main = "Eigenvalues")
res.mca$var$coord
res.mca$var$contrib
res.mca$var$cos2
res.mca$var$v.test
dimdesc(res.mca)
# Chunk 4
res.hcpc <- HCPC(res.mca, nb.clust=-1)
res.hcpc$desc.axes
res.hcpc$call$t$nb.clust
barplot(res.hcpc$call$t$inert.gain[1:10], type = "b")
# Dendrogram and cluster visualisation using fviz
fviz_dend(res.hcpc, rect = TRUE, rect_fill = TRUE)
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
# Chunk 5
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2))
pca_result$eig
hcpc_result <- HCPC(pca_result,graph=FALSE)
hcpc_result$call$t$nb.clust #3 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
length(which(hcpc_result$data.clust$clust == 3))
# Set the minimum number of clusters to 2
hcpc_result <- HCPC(pca_result, min=2, nb.clust = -1)
hcpc_result$call$t$nb.clust # 2 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
fviz_dend(hcpc_result, rect = TRUE, rect_fill = TRUE)
fviz_cluster(hcpc_result, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
#Limit PCA to the first 2 dimensions
pca_result <- PCA(df[, c(1:8)], quali.sup = c(1, 2), ncp=2)
hcpc_result <- HCPC(pca_result, min=2, nb.clust = -1)
hcpc_result$call$t$nb.clust # 4 clusters
length(which(hcpc_result$data.clust$clust == 1))
length(which(hcpc_result$data.clust$clust == 2))
length(which(hcpc_result$data.clust$clust == 3))
length(which(hcpc_result$data.clust$clust == 4))
fviz_dend(hcpc_result, rect = TRUE, rect_fill = TRUE)
# Chunk 6
fviz_cluster(res.hcpc, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
length(which(res.hcpc$data.clust$clust == 2 & res.hcpc$data.clust$Channel == "Channel_1" ))
length(which(res.hcpc$data.clust$clust == 2 & res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$clust == 3 & res.hcpc$data.clust$Channel == "Channel_1" ))
length(which(res.hcpc$data.clust$clust == 3 & res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$Channel == "Channel_2" ))
length(which(res.hcpc$data.clust$clust == 4 & res.hcpc$data.clust$Channel == "Channel_1" ))
res.hcpc$desc.var
res.hcpc$desc.var$category
res.hcpc$data.clust
# Chunk 7
fviz_cluster(hcpc_result, repel = TRUE, show.clust.cent = TRUE,main = "Factor map")
hcpc_result$data.clust
hcpc_result$desc.var
hcpc_result$desc.axes
hcpc_result$desc.ind
hcpc_result$call
